{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (1.45.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: certifi in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/devjpt23/grok-integrate/kai/env/lib64/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "%pip install python-dotenv\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test multiple prompts by estimating the tokens prior to sending them to the LLM. Then, we will get the actual tokens, and thereafter, we can find the percentage difference to see how far off our estimate is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    1: \"The sky turned a deep shade of orange as the sun dipped below the horizon.\",\n",
    "    2: \"She couldn't believe how quickly the day had passed, feeling like it was only minutes ago she had woken up.\",\n",
    "    3: \"The cat leaped gracefully onto the windowsill, watching the world outside with curious eyes.\",\n",
    "    4: \"He carefully arranged the books on the shelf, ensuring they were organized by size and color.\",\n",
    "    5: \"The gentle hum of the air conditioner was the only sound in the otherwise quiet room.\",\n",
    "    6: \"They planned to meet at the coffee shop at noon, eager to catch up after months apart.\",\n",
    "    7: \"The scent of fresh-baked bread filled the kitchen, making everyone's mouth water.\",\n",
    "    8: \"A sudden gust of wind scattered the papers on her desk, sending them flying across the room.\",\n",
    "    9: \"He struggled to solve the puzzle, but the answer seemed just out of reach.\",\n",
    "    10: \"The old clock in the hallway chimed, marking the start of a new hour.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimating_tokens(prompt):\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    estimate_prompt_tokens = len(enc.encode(prompt))\n",
    "    return int(estimate_prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_tokens_func(prompt):\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # conversion to json\n",
    "    json_response = json.loads(response.model_dump_json())\n",
    "    actual_token_info = json_response['usage']['prompt_tokens']\n",
    "    return int(actual_token_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_off_list =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating tokens 16 in prompt number 1\n",
      "Actual tokens 23 in prompt number 1\n",
      "43\n",
      "Estimating tokens 23 in prompt number 2\n",
      "Actual tokens 30 in prompt number 2\n",
      "30\n",
      "Estimating tokens 18 in prompt number 3\n",
      "Actual tokens 25 in prompt number 3\n",
      "38\n",
      "Estimating tokens 18 in prompt number 4\n",
      "Actual tokens 25 in prompt number 4\n",
      "38\n",
      "Estimating tokens 17 in prompt number 5\n",
      "Actual tokens 24 in prompt number 5\n",
      "41\n",
      "Estimating tokens 19 in prompt number 6\n",
      "Actual tokens 26 in prompt number 6\n",
      "36\n",
      "Estimating tokens 16 in prompt number 7\n",
      "Actual tokens 23 in prompt number 7\n",
      "43\n",
      "Estimating tokens 19 in prompt number 8\n",
      "Actual tokens 26 in prompt number 8\n",
      "36\n",
      "Estimating tokens 16 in prompt number 9\n",
      "Actual tokens 23 in prompt number 9\n",
      "43\n",
      "Estimating tokens 17 in prompt number 10\n",
      "Actual tokens 24 in prompt number 10\n",
      "41\n",
      "[43, 30, 38, 38, 41, 36, 43, 36, 43, 41]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for prompt_number,prompt in (prompts.items()):\n",
    "    print(f\"Estimating tokens {estimating_tokens(prompt)} in prompt number {prompt_number}\")\n",
    "    print(f\"Actual tokens {actual_tokens_func(prompt)} in prompt number {prompt_number}\")\n",
    "\n",
    "    estimated_tokens = estimating_tokens(prompt)\n",
    "    actual_tokens = actual_tokens_func(prompt)\n",
    "\n",
    "    difference = int(actual_tokens) - int(estimated_tokens)\n",
    "    percentage_off = int((int(difference) / int(estimated_tokens)) * 100)\n",
    "    print(percentage_off)\n",
    "    percentage_off_list.append(percentage_off)\n",
    "\n",
    "print(percentage_off_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_percentage():\n",
    "    added = 0\n",
    "    for i in percentage_off_list:\n",
    "        added += i\n",
    "    return(added/len(percentage_off_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the results, we can see that we have an average of 38.9 for the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE PERCENTAGE DIFFERENCE: 38.9%\n"
     ]
    }
   ],
   "source": [
    "print(f\"AVERAGE PERCENTAGE DIFFERENCE: {average_percentage()}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
